# LangChain
framework for developing apps powered by LLMs

# **LangChain Ecosystem:**
 >    1. **LangSmith**: for observability
 >    2. **LangServe**: for deploying chains as APIs
 >    3. **Templates**: Reference Applications
 >    4. **LangChain: Chain / Agents / Advanced Retrieval Strategies**
 >    5. **LangChain Community**: Integrations/Components
                    Models I/O
                    Retrieval
                    Agent Tooling
 >    6. **LangChain Core**: Protocol
                    LCEL - LangChain Expression Language

# Other Useful Frameworks, Libraries, Tools:
 <ol>
  <li>Ollama: (Omni-Layer Learning Language Acquisition Model)
      <p>to run LLM on local servers aka. #DockerForLLMs supports various libraries such as LLama2, Claude, Mistral, Dolphin Phi, and LLaVa (cmd: "ollama run modelname") </p> </li>
  <li>Streamlit: to turn DS, ML apps into web applications (cmd: "streamlit run filename.py") </li>
  <li>Fast API: to perform production-grade deployment of LLM as API. Perform Swagger UI Documentation.</li>
  <li>Uvicorn: an Asynchronous Server Gateway Interface (ASGI) Server which acts as binding element that handles web connections from browser or api client and then allows FastAPI to serve actual request (cmd: uvicorn.run())  </li>
 </ol>
